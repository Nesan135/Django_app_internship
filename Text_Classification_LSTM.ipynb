{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfXB0RK5Vjq+YEZhzKF+2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nesan135/Django_app_internship/blob/master/Text_Classification_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc31V4KULOW7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow, unicodedata, re, contractions, string, spacy, time, textwrap, os, datetime, pickle, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
        "from tensorflow.keras import Sequential\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "CSV_PATH = os.path.join(os.get_cwd(), 'Dataset', 'True.csv')\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# EDA\n",
        "print(df.info())\n",
        "print(df.head(5))\n",
        "print(df.duplicated().sum())\n",
        "print(df.isna().sum())\n",
        "print(df.columns)\n",
        "print(df['subject'].unique())\n",
        "df.drop_duplicates()\n",
        "\n",
        "# Observations : No null data, contains duplicates (206), all data are strings, values to predict : ['politicsNews' 'worldnews']\n",
        "\n",
        "# Cleaning\n",
        "def expand_contractions(text):\n",
        "    expanded_words = [] \n",
        "    for word in text.split():\n",
        "       expanded_words.append(contractions.fix(word)) \n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "def lemmatize(text, nlp):\n",
        "   doc = nlp(text)\n",
        "   lemmatized_text = []\n",
        "   for token in doc:\n",
        "     lemmatized_text.append(token.lemma_)\n",
        "   return ' '.join(lemmatized_text)\n",
        "\n",
        "def remove_stopwords(text,nlp):          \n",
        "    filtered_sentence = [] \n",
        "    doc = nlp(text)\n",
        "    for token in doc:        \n",
        "        if token.is_stop == False: \n",
        "          filtered_sentence.append(token.text)   \n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "fake_commas = [['“','\"'],['”','\"'],['‘',\"'\"],['’',\"'\"]]\n",
        "start = time.time()\n",
        "counter = 0\n",
        "\n",
        "for index,data in enumerate(df['text']):\n",
        "    # Standardizing Accent Characters\n",
        "    data = unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    # remove false commas\n",
        "    for fake_comma in fake_commas:\n",
        "        data = re.sub(fake_comma[0],fake_comma[1],data)    \n",
        "    # remove tags\n",
        "    data = re.sub(r'@\\S*', '', data)\n",
        "    # remove HTML tags\n",
        "    data = re.sub('<.*?>','', data)\n",
        "    # remove URLS\n",
        "    data = re.sub(r'bit.ly?:\\S*', '', data)\n",
        "    # remove special char, numbers and lower case\n",
        "    data = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', ' ', data).lower()\n",
        "    # expand contractions\n",
        "    data = expand_contractions(data)\n",
        "    # remove punctuation\n",
        "    data = ''.join([c for c in data if c not in string.punctuation])\n",
        "    data = lemmatize(data,nlp)\n",
        "    data = remove_stopwords(data,nlp)\n",
        "    # #to check :\n",
        "    counter +=1\n",
        "    if counter%1000 == 0:\n",
        "        end = time.time()\n",
        "        print(counter,end-start)\n",
        "        start = end\n",
        "    # commit to dataframe\n",
        "    df['text'][index] = data\n",
        "\n",
        "# feature selection\n",
        "review = df['text']\n",
        "sentiment = df['subject']\n",
        "\n",
        "# unique number of words in all sentences\n",
        "num_words = 5000\n",
        "\n",
        "# out of vocab\n",
        "oov_token = '<OOV>'\n",
        "tokenizer = Tokenizer(num_words=num_words,oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(review)\n",
        "# word_index = tokenizer.word_index\n",
        "# print(dict(list(word_index.items())[0:10]))\n",
        "\n",
        "# preprocessing\n",
        "review = tokenizer.texts_to_sequences(review)\n",
        "padded_review = pad_sequences(review, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "sentiment = ohe.fit_transform(sentiment[::,None])\n",
        "\n",
        "padded_review = np.expand_dims(padded_review, axis=-1)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(padded_review,sentiment,test_size=0.2,random_state=123)\n",
        "\n",
        "embedding_layer = 64\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_layer))\n",
        "model.add(LSTM(embedding_layer, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "log_path = os.path.join('log_dir','time_series',datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tb_callback = TensorBoard(log_dir=log_path)\n",
        "es_callback = EarlyStopping(monitor='val_loss',patience=5,verbose=0,restore_best_weights=True)\n",
        "model_callback = ModelCheckpoint('best_weights.h5', monitor='val_loss', save_best_only='True', verbose=1)\n",
        "hist = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=40,batch_size=64, callbacks=[es_callback,tb_callback,model_callback])\n",
        "\n",
        "print('hist keys :',hist.history.keys())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.legend(['training','validation'])\n",
        "plt.show()\n",
        "\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "y_predicted = model.predict(X_test)\n",
        "y_predicted = np.argmax(y_predicted, axis=1)\n",
        "print(classification_report(y_test, y_predicted))\n",
        "print(confusion_matrix(y_test, y_predicted))\n",
        "\n",
        "# model saving\n",
        "model.save('model.h5')\n",
        "with open('ohe.pkl', 'wb') as f:\n",
        "    pickle.dump(ohe,f)\n",
        "\n",
        "token_json = tokenizer.to_json()\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    json.dump(token_json,f)"
      ]
    }
  ]
}